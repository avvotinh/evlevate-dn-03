"""
RAG Generation Tool for E-commerce AI Product Advisor
Generates natural language responses using retrieved context from vector search.
"""

import json
from typing import Dict, List, Optional, Any, Type
from pydantic import BaseModel, Field

from langchain.tools import BaseTool
from langchain_core.callbacks import CallbackManagerForToolRun

from src.services.llm_service import LLMService
from src.prompts.prompt_manager import prompt_manager, PromptType
from src.utils.logger import get_logger

logger = get_logger("rag_generation_tool")


class RAGGenerationInput(BaseModel):
    """Input schema for RAGGenerationTool"""
    user_query: str = Field(
        description="Original user query in Vietnamese"
    )
    context: Optional[str] = Field(
        default="",
        description="Retrieved context from search results (products, reviews, etc.)"
    )
    conversation_history: Optional[str] = Field(
        default="",
        description="Previous conversation context"
    )
    response_type: Optional[str] = Field(
        default="general",
        description="Type of response: 'general', 'comparison', 'recommendation', 'explanation'"
    )


class RAGGenerationTool(BaseTool):
    """Tool for generating natural language responses using retrieved context"""
    
    name: str = "answer_with_context"
    description: str = """Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn dá»±a trÃªn thÃ´ng tin sáº£n pháº©m Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c.
    
    Sá»­ dá»¥ng tool nÃ y khi:
    - ÄÃ£ cÃ³ thÃ´ng tin sáº£n pháº©m tá»« search/filter tools
    - Cáº§n táº¡o cÃ¢u tráº£ lá»i chi tiáº¿t vÃ  tá»± nhiÃªn
    - Muá»‘n giáº£i thÃ­ch hoáº·c tÆ° váº¥n dá»±a trÃªn context
    - Cáº§n tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n
    
    Tool sáº½ táº¡o ra cÃ¢u tráº£ lá»i hoÃ n chá»‰nh, thÃ¢n thiá»‡n vÃ  há»¯u Ã­ch."""
    
    args_schema: Type[BaseModel] = RAGGenerationInput
    return_direct: bool = False
    
    def _run(
        self,
        user_query: str,
        context: Optional[str] = "",
        conversation_history: Optional[str] = "",
        response_type: Optional[str] = "general",
        run_manager: Optional[CallbackManagerForToolRun] = None,
    ) -> str:
        """Execute the RAG generation tool"""
        try:
            logger.info(f"ðŸ¤– Generating response for query: '{user_query[:50]}...'")
            
            # Validate inputs
            if not user_query or not user_query.strip():
                return json.dumps({
                    "success": False,
                    "error": "User query khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng",
                    "response": ""
                })
            
            # Context is optional - if empty, treat as general knowledge query
            if not context:
                context = ""
            
            # Build prompt based on response type
            system_prompt = prompt_manager.build_rag_system_prompt(response_type)
            user_prompt = prompt_manager.build_rag_user_prompt(user_query, context, conversation_history)
            
            # Generate response using LLM
            llm_service = LLMService()
            llm = llm_service.get_llm()
            
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            response = llm.invoke(full_prompt)
            
            # Extract content if response is AIMessage
            if hasattr(response, 'content'):
                generated_response = response.content
            else:
                generated_response = str(response)
            
            result = {
                "success": True,
                "response": generated_response.strip(),
                "response_type": response_type,
                "context_used": len(context) > 0,
                "query": user_query
            }
            
            logger.info("âœ… RAG response generated successfully")
            return json.dumps(result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"âŒ RAG generation error: {e}")
            return json.dumps({
                "success": False,
                "error": f"Lá»—i táº¡o cÃ¢u tráº£ lá»i: {str(e)}",
                "response": "Xin lá»—i, tÃ´i gáº·p váº¥n Ä‘á» khi táº¡o cÃ¢u tráº£ lá»i. Vui lÃ²ng thá»­ láº¡i."
            })
    
    async def _arun(
        self,
        user_query: str,
        context: Optional[str] = "",
        conversation_history: Optional[str] = "",
        response_type: Optional[str] = "general",
        run_manager: Optional[CallbackManagerForToolRun] = None,
    ) -> str:
        """Async version of the tool"""
        return self._run(user_query, context, conversation_history, response_type, run_manager)


# Create tool instance
rag_generation_tool = RAGGenerationTool()
