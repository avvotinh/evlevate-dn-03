"""
Enhanced LangGraph Agent for E-commerce AI Product Advisor
Improved implementation with ReAct Agent compatibility and advanced features
"""

from typing import Dict, List, Any, Optional, TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain.memory import ConversationBufferWindowMemory
import json
import re

from src.services.llm_service import llm_service
from src.tools.tool_manager import ToolManager
from src.utils.logger import get_logger

logger = get_logger("langgraph_agent")


class AgentState(TypedDict):
    """Enhanced state for LangGraph agent"""
    messages: Annotated[List[BaseMessage], add_messages]
    user_input: str
    session_id: Optional[str]
    current_step: str
    intent: str
    tools_used: List[str]
    search_results: Optional[Dict[str, Any]]
    comparison_results: Optional[Dict[str, Any]]
    recommendation_results: Optional[Dict[str, Any]]
    context_data: Optional[str]
    final_response: Optional[str]
    iteration_count: int
    error_count: int
    reasoning_steps: List[Dict[str, Any]]


class ProductAdvisorLangGraphAgent:
    """Enhanced LangGraph-based Product Advisor Agent"""
    
    def __init__(self):
        """Initialize LangGraph agent with ReAct compatibility"""
        self.tool_manager = ToolManager()
        self.llm = llm_service.get_llm()
        
        # Memory management similar to ReAct agent
        self.memories: Dict[str, ConversationBufferWindowMemory] = {}
        
        self.graph = self._create_graph()
        logger.info("‚úÖ Enhanced LangGraph Agent initialized")
    
    def _get_memory(self, session_id: str) -> ConversationBufferWindowMemory:
        """Get or create memory for session"""
        if session_id not in self.memories:
            self.memories[session_id] = ConversationBufferWindowMemory(
                k=10, memory_key="chat_history", return_messages=True,
                output_key="output", input_key="input"
            )
        return self.memories[session_id]
    
    def _create_graph(self) -> StateGraph:
        """Create enhanced LangGraph workflow"""
        
        workflow = StateGraph(AgentState)
        
        # Add nodes with error handling
        workflow.add_node("analyze_intent", self._analyze_intent)
        workflow.add_node("handle_greeting", self._handle_greeting)
        workflow.add_node("search_products", self._search_products)
        workflow.add_node("compare_products", self._compare_products)
        workflow.add_node("recommend_products", self._recommend_products)
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("handle_error", self._handle_error)
        
        # Define entry point
        workflow.set_entry_point("analyze_intent")
        
        # Enhanced conditional routing
        workflow.add_conditional_edges(
            "analyze_intent",
            self._route_intent,
            {
                "greeting": "handle_greeting",
                "search": "search_products",
                "compare": "compare_products", 
                "recommend": "recommend_products",
                "direct": "generate_response",
                "error": "handle_error"
            }
        )
        
        # All nodes can lead to response generation or error handling
        workflow.add_edge("handle_greeting", "generate_response")
        workflow.add_edge("search_products", "generate_response")
        workflow.add_edge("compare_products", "generate_response")
        workflow.add_edge("recommend_products", "generate_response")
        workflow.add_edge("handle_error", "generate_response")
        workflow.add_edge("generate_response", END)
        
        return workflow.compile()

    def _classify_intent_with_llm(self, user_input: str) -> str:
        """Use LLM to classify user intent intelligently"""
        try:
            intent_prompt = f"""Ph√¢n t√≠ch √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ CH√çNH X√ÅC m·ªôt trong c√°c intent sau:

INTENT OPTIONS:
- greeting: Ch√†o h·ªèi, c·∫£m ∆°n, h·ªèi v·ªÅ AI
- search: T√¨m ki·∫øm th√¥ng tin s·∫£n ph·∫©m c·ª• th·ªÉ, h·ªèi c·∫•u h√¨nh, th√¥ng s·ªë, gi√°
- compare: So s√°nh 2+ s·∫£n ph·∫©m
- recommend: Xin g·ª£i √Ω, t∆∞ v·∫•n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu
- direct: C√¢u h·ªèi ƒë∆°n gi·∫£n kh√°c

USER INPUT: "{user_input}"

EXAMPLES:
- "Xin ch√†o" ‚Üí greeting
- "Dell Inspiron 14 5420 c·∫•u h√¨nh nh∆∞ th·∫ø n√†o" ‚Üí search
- "iPhone 15 vs Samsung S24" ‚Üí compare
- "G·ª£i √Ω laptop cho sinh vi√™n" ‚Üí recommend
- "C·∫£m ∆°n b·∫°n" ‚Üí greeting

Ch·ªâ tr·∫£ v·ªÅ T√äN INTENT (greeting/search/compare/recommend/direct):"""

            response = self.llm.invoke([HumanMessage(content=intent_prompt)])
            intent = response.content.strip().lower()

            # Validate intent
            valid_intents = ["greeting", "search", "compare", "recommend", "direct"]
            if intent in valid_intents:
                logger.info(f"ü§ñ LLM classified intent: {intent}")
                return intent
            else:
                logger.warning(f"‚ö†Ô∏è LLM returned invalid intent: {intent}")
                return None

        except Exception as e:
            logger.error(f"‚ùå LLM intent classification failed: {e}")
            return None

    def _classify_intent_with_rules(self, user_input: str) -> str:
        """Fallback rule-based intent classification"""
        try:
            # Greeting detection
            if any(word in user_input for word in ["xin ch√†o", "hello", "ch√†o", "hi", "c·∫£m ∆°n", "thank"]):
                return "greeting"
            # Comparison intent
            elif any(word in user_input for word in ["so s√°nh", "compare", "kh√°c nhau", "vs", "versus"]):
                return "compare"
            # Recommendation intent
            elif any(word in user_input for word in ["g·ª£i √Ω", "recommend", "n√™n mua", "ph√π h·ª£p", "t∆∞ v·∫•n"]):
                return "recommend"
            # Search intent - Enhanced to catch product-specific queries
            elif any(word in user_input for word in [
                "t√¨m", "search", "laptop", "smartphone", "ƒëi·ªán tho·∫°i", "macbook", "dell", "hp", "asus",
                "iphone", "samsung", "xiaomi", "oppo", "vivo", "c·∫•u h√¨nh", "th√¥ng s·ªë", "gi√°", "specs",
                "inspiron", "thinkpad", "pavilion", "vivobook", "galaxy", "redmi", "nh∆∞ th·∫ø n√†o", "ra sao"
            ]):
                return "search"
            # Direct response for simple queries
            else:
                return "direct"

        except Exception as e:
            logger.error(f"‚ùå Rule-based intent classification failed: {e}")
            return "direct"

    def _analyze_intent(self, state: AgentState) -> AgentState:
        """LLM-powered intent analysis for better accuracy"""
        user_input = state["user_input"]

        try:
            # Use LLM for intelligent intent classification
            intent = self._classify_intent_with_llm(user_input)
            classification_method = "LLM"

            # Fallback to rule-based if LLM fails
            if not intent:
                intent = self._classify_intent_with_rules(user_input.lower())
                classification_method = "Rules"

            state["intent"] = intent
            state["current_step"] = intent

            # Add reasoning step with classification method info
            reasoning_step = {
                "step": len(state.get("reasoning_steps", [])) + 1,
                "action": "analyze_intent",
                "thought": f"Ph√¢n t√≠ch √Ω ƒë·ªãnh ng∆∞·ªùi d√πng ({classification_method}): '{user_input[:50]}...' -> {intent}",
                "result": intent,
                "method": classification_method
            }
            if "reasoning_steps" not in state:
                state["reasoning_steps"] = []
            state["reasoning_steps"].append(reasoning_step)

            logger.info(f"üéØ Intent analyzed ({classification_method}): {intent}")

        except Exception as e:
            logger.error(f"‚ùå Intent analysis error: {e}")
            state["intent"] = "error"
            state["current_step"] = "error"
            state["error_count"] = state.get("error_count", 0) + 1

        return state
    
    def _route_intent(self, state: AgentState) -> str:
        """Enhanced routing with error handling"""
        return state.get("current_step", "error")
    
    def _handle_greeting(self, state: AgentState) -> AgentState:
        """Handle greeting messages"""
        try:
            greeting_response = """Xin ch√†o! üëã T√¥i l√† AI Product Advisor - tr·ª£ l√Ω AI chuy√™n t∆∞ v·∫•n s·∫£n ph·∫©m ƒëi·ªán t·ª≠.

T√¥i c√≥ th·ªÉ gi√∫p b·∫°n:
üîç T√¨m ki·∫øm laptop v√† smartphone ph√π h·ª£p
‚öñÔ∏è So s√°nh s·∫£n ph·∫©m chi ti·∫øt  
üí° ƒê∆∞a ra g·ª£i √Ω d·ª±a tr√™n nhu c·∫ßu
üí∞ T∆∞ v·∫•n theo ng√¢n s√°ch

B·∫°n ƒëang t√¨m s·∫£n ph·∫©m g√¨ h√¥m nay?"""
            
            state["final_response"] = greeting_response
            state["tools_used"].append("handle_greeting")
            
            # Add reasoning step
            reasoning_step = {
                "step": len(state.get("reasoning_steps", [])) + 1,
                "action": "handle_greeting",
                "thought": "Ng∆∞·ªùi d√πng ch√†o h·ªèi, t√¥i s·∫Ω tr·∫£ l·ªùi th√¢n thi·ªán v√† gi·ªõi thi·ªáu d·ªãch v·ª•",
                "result": "greeting_completed"
            }
            state["reasoning_steps"].append(reasoning_step)
            
            logger.info("‚úÖ Greeting handled")
            
        except Exception as e:
            logger.error(f"‚ùå Greeting handling failed: {e}")
            state["final_response"] = "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
            state["error_count"] = state.get("error_count", 0) + 1
        
        return state
    
    def _extract_search_query(self, user_input: str) -> str:
        """Extract search query from user input using LLM"""
        try:
            extraction_prompt = f"""
T·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng sau, h√£y tr√≠ch xu·∫•t t·ª´ kh√≥a t√¨m ki·∫øm ch√≠nh:

C√¢u h·ªèi: "{user_input}"

H√£y tr·∫£ v·ªÅ ch·ªâ t·ª´ kh√≥a t√¨m ki·∫øm (kh√¥ng gi·∫£i th√≠ch th√™m), v√≠ d·ª•:
- "laptop cho l·∫≠p tr√¨nh"
- "ƒëi·ªán tho·∫°i gaming"
- "m√°y t√≠nh vƒÉn ph√≤ng"

T·ª´ kh√≥a t√¨m ki·∫øm:"""

            response = self.llm.invoke(extraction_prompt)
            extracted_query = response.content.strip().strip('"').strip("'")

            # Fallback to original input if extraction fails
            if not extracted_query or len(extracted_query) < 3:
                extracted_query = user_input

            logger.info(f"üîç Extracted search query: '{extracted_query}' from '{user_input[:50]}...'")
            return extracted_query

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Query extraction failed: {e}, using original input")
            return user_input

    def _search_products(self, state: AgentState) -> AgentState:
        """Enhanced product search with error handling"""
        try:
            search_tool = self.tool_manager.get_tool("search")
            if not search_tool:
                raise Exception("Search tool not available")

            # Extract specific search query from user input
            search_query = self._extract_search_query(state["user_input"])

            result = search_tool.run(search_query)
            state["search_results"] = result
            state["tools_used"].append("search_products")
            
            # Add reasoning step
            reasoning_step = {
                "step": len(state.get("reasoning_steps", [])) + 1,
                "action": "search_products",
                "thought": f"T√¨m ki·∫øm s·∫£n ph·∫©m v·ªõi query: '{search_query}' (extracted from: '{state['user_input'][:50]}...')",
                "action_input": search_query,
                "observation": str(result)[:100] + "..." if len(str(result)) > 100 else str(result)
            }
            state["reasoning_steps"].append(reasoning_step)
            
            logger.info("‚úÖ Search completed")
            
        except Exception as e:
            logger.error(f"‚ùå Search failed: {e}")
            state["search_results"] = {"error": str(e), "success": False}
            state["error_count"] = state.get("error_count", 0) + 1
        
        return state

    def _extract_compare_query(self, user_input: str) -> str:
        """Extract comparison query from user input using LLM"""
        try:
            extraction_prompt = f"""
T·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng sau, h√£y tr√≠ch xu·∫•t th√¥ng tin ƒë·ªÉ so s√°nh s·∫£n ph·∫©m:

C√¢u h·ªèi: "{user_input}"

N·∫øu c√≥ t√™n s·∫£n ph·∫©m c·ª• th·ªÉ, h√£y tr·∫£ v·ªÅ t√™n s·∫£n ph·∫©m. N·∫øu kh√¥ng, h√£y tr·∫£ v·ªÅ t·ª´ kh√≥a so s√°nh ch√≠nh:

V√≠ d·ª•:
- "iPhone 15, Samsung Galaxy S24"
- "laptop Dell vs HP"
- "so s√°nh ƒëi·ªán tho·∫°i gaming"

Th√¥ng tin so s√°nh:"""

            response = self.llm.invoke(extraction_prompt)
            extracted_query = response.content.strip().strip('"').strip("'")

            # Fallback to original input if extraction fails
            if not extracted_query or len(extracted_query) < 3:
                extracted_query = user_input

            logger.info(f"üîç Extracted compare query: '{extracted_query}' from '{user_input[:50]}...'")
            return extracted_query

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Compare query extraction failed: {e}, using original input")
            return user_input

    def _compare_products(self, state: AgentState) -> AgentState:
        """Enhanced product comparison with error handling"""
        try:
            compare_tool = self.tool_manager.get_tool("compare")
            if not compare_tool:
                raise Exception("Compare tool not available")

            # Extract specific comparison query from user input
            compare_query = self._extract_compare_query(state["user_input"])

            result = compare_tool.run(compare_query)
            state["comparison_results"] = result
            state["tools_used"].append("compare_products")

            # Add reasoning step
            reasoning_step = {
                "step": len(state.get("reasoning_steps", [])) + 1,
                "action": "compare_products",
                "thought": f"So s√°nh s·∫£n ph·∫©m v·ªõi query: '{compare_query}' (extracted from: '{state['user_input'][:50]}...')",
                "action_input": compare_query,
                "observation": str(result)[:100] + "..." if len(str(result)) > 100 else str(result)
            }
            state["reasoning_steps"].append(reasoning_step)

            logger.info("‚úÖ Comparison completed")

        except Exception as e:
            logger.error(f"‚ùå Comparison failed: {e}")
            state["comparison_results"] = {"error": str(e), "success": False}
            state["error_count"] = state.get("error_count", 0) + 1

        return state

    def _extract_recommend_query(self, user_input: str) -> str:
        """Extract recommendation query from user input using LLM"""
        try:
            extraction_prompt = f"""
T·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng sau, h√£y tr√≠ch xu·∫•t nhu c·∫ßu v√† y√™u c·∫ßu ƒë·ªÉ g·ª£i √Ω s·∫£n ph·∫©m:

C√¢u h·ªèi: "{user_input}"

H√£y tr·∫£ v·ªÅ m√¥ t·∫£ nhu c·∫ßu ng·∫Øn g·ªçn, bao g·ªìm:
- Lo·∫°i s·∫£n ph·∫©m c·∫ßn
- M·ª•c ƒë√≠ch s·ª≠ d·ª•ng
- Ng√¢n s√°ch (n·∫øu c√≥)
- Y√™u c·∫ßu ƒë·∫∑c bi·ªát (n·∫øu c√≥)

V√≠ d·ª•:
- "sinh vi√™n IT c·∫ßn laptop l·∫≠p tr√¨nh ng√¢n s√°ch 20 tri·ªáu"
- "ƒëi·ªán tho·∫°i ch·ª•p ·∫£nh ƒë·∫πp d∆∞·ªõi 15 tri·ªáu"
- "laptop gaming cao c·∫•p"

Nhu c·∫ßu:"""

            response = self.llm.invoke(extraction_prompt)
            extracted_query = response.content.strip().strip('"').strip("'")

            # Fallback to original input if extraction fails
            if not extracted_query or len(extracted_query) < 3:
                extracted_query = user_input

            logger.info(f"üîç Extracted recommend query: '{extracted_query}' from '{user_input[:50]}...'")
            return extracted_query

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Recommend query extraction failed: {e}, using original input")
            return user_input

    def _recommend_products(self, state: AgentState) -> AgentState:
        """Enhanced product recommendation with error handling"""
        try:
            recommend_tool = self.tool_manager.get_tool("recommend")
            if not recommend_tool:
                raise Exception("Recommend tool not available")

            # Extract specific recommendation query from user input
            recommend_query = self._extract_recommend_query(state["user_input"])

            result = recommend_tool.run(recommend_query)
            state["recommendation_results"] = result
            state["tools_used"].append("recommend_products")

            # Add reasoning step
            reasoning_step = {
                "step": len(state.get("reasoning_steps", [])) + 1,
                "action": "recommend_products",
                "thought": f"G·ª£i √Ω s·∫£n ph·∫©m v·ªõi nhu c·∫ßu: '{recommend_query}' (extracted from: '{state['user_input'][:50]}...')",
                "action_input": recommend_query,
                "observation": str(result)[:100] + "..." if len(str(result)) > 100 else str(result)
            }
            state["reasoning_steps"].append(reasoning_step)

            logger.info("‚úÖ Recommendation completed")

        except Exception as e:
            logger.error(f"‚ùå Recommendation failed: {e}")
            state["recommendation_results"] = {"error": str(e), "success": False}
            state["error_count"] = state.get("error_count", 0) + 1

        return state

    def _generate_response(self, state: AgentState) -> AgentState:
        """Enhanced response generation with ReAct-style RAG"""
        try:
            # Skip if response already generated (e.g., greeting)
            if state.get("final_response"):
                return state

            # Collect context from all results
            context_parts = []

            # Check search results
            if state.get("search_results"):
                search_result = state["search_results"]
                if isinstance(search_result, dict) and not search_result.get("error"):
                    context_parts.append(f"Search results: {search_result}")
                elif isinstance(search_result, str):
                    context_parts.append(f"Search results: {search_result}")

            # Check comparison results
            if state.get("comparison_results"):
                compare_result = state["comparison_results"]
                if isinstance(compare_result, dict) and not compare_result.get("error"):
                    context_parts.append(f"Comparison: {compare_result}")
                elif isinstance(compare_result, str):
                    context_parts.append(f"Comparison: {compare_result}")

            # Check recommendation results
            if state.get("recommendation_results"):
                recommend_result = state["recommendation_results"]
                if isinstance(recommend_result, dict) and not recommend_result.get("error"):
                    context_parts.append(f"Recommendations: {recommend_result}")
                elif isinstance(recommend_result, str):
                    context_parts.append(f"Recommendations: {recommend_result}")

            # Use RAG generation tool if we have context
            if context_parts:
                context = "\n".join(context_parts)
                state["context_data"] = context

                rag_tool = self.tool_manager.get_tool("answer_with_context")
                if rag_tool:
                    rag_input = {
                        "user_query": state["user_input"],
                        "context": context,
                        "response_type": "general"
                    }

                    result = rag_tool.run(rag_input)

                    # Parse RAG tool result safely
                    if isinstance(result, dict):
                        if result.get("success"):
                            state["final_response"] = result.get("response", "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi")
                        else:
                            state["final_response"] = result.get("response", str(result))
                    elif isinstance(result, str):
                        # Try to parse JSON response
                        try:
                            result_data = json.loads(result)
                            if isinstance(result_data, dict):
                                state["final_response"] = result_data.get("response", result)
                            else:
                                state["final_response"] = result
                        except json.JSONDecodeError:
                            state["final_response"] = result
                    else:
                        state["final_response"] = str(result)

                    # Add reasoning step
                    reasoning_step = {
                        "step": len(state.get("reasoning_steps", [])) + 1,
                        "action": "answer_with_context",
                        "thought": "S·ª≠ d·ª•ng RAG ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ context ƒë√£ thu th·∫≠p",
                        "action_input": str(rag_input),
                        "observation": state["final_response"][:100] + "..." if len(state["final_response"]) > 100 else state["final_response"]
                    }
                    state["reasoning_steps"].append(reasoning_step)

                    state["tools_used"].append("answer_with_context")
                else:
                    raise Exception("RAG tool not available")
            else:
                # Direct LLM response for simple queries
                response = self.llm.invoke([HumanMessage(content=state["user_input"])])
                state["final_response"] = response.content

            logger.info("‚úÖ Response generated")

        except Exception as e:
            logger.error(f"‚ùå Response generation failed: {e}")
            state["final_response"] = self._get_error_response(str(e))
            state["error_count"] = state.get("error_count", 0) + 1

        return state

    def _handle_error(self, state: AgentState) -> AgentState:
        """Handle errors gracefully"""
        error_msg = "Xin l·ªói, t√¥i g·∫∑p kh√≥ khƒÉn trong vi·ªác x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i ho·∫∑c di·ªÖn ƒë·∫°t kh√°c kh√¥ng?"
        state["final_response"] = error_msg
        logger.error(f"‚ùå Error handled for session: {state.get('session_id', 'unknown')}")
        return state

    def _get_error_response(self, error: str) -> str:
        """Get user-friendly error response"""
        if "setup_required" in error or "NOT_FOUND" in error:
            return "‚ùå **C∆° s·ªü d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p**\n\nüîß **C√°ch kh·∫Øc ph·ª•c:**\n1. Ch·∫°y l·ªánh: `python scripts/insert_sample_data.py`\n2. ƒê·∫£m b·∫£o Pinecone index 'ecommerce-products' ƒë√£ ƒë∆∞·ª£c t·∫°o\n\nüí° Sau khi thi·∫øt l·∫≠p xong, h√£y th·ª≠ l·∫°i c√¢u h·ªèi c·ªßa b·∫°n!"
        else:
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {error}. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?"

    def chat(self, user_input: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Enhanced chat method with ReAct compatibility"""
        try:
            if not session_id:
                session_id = "default"

            logger.info(f"üó£Ô∏è Processing user input: {user_input[:100]}...")

            # Get memory for session
            memory = self._get_memory(session_id)

            # Initialize enhanced state
            initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "user_input": user_input,
                "session_id": session_id,
                "current_step": "",
                "intent": "",
                "tools_used": [],
                "search_results": None,
                "comparison_results": None,
                "recommendation_results": None,
                "context_data": None,
                "final_response": None,
                "iteration_count": 0,
                "error_count": 0,
                "reasoning_steps": []
            }

            # Run the graph
            final_state = self.graph.invoke(initial_state)

            # Update memory
            memory.save_context(
                {"input": user_input},
                {"output": final_state.get("final_response", "")}
            )

            # Extract intermediate steps for compatibility
            intermediate_steps = []
            for step in final_state.get("reasoning_steps", []):
                if step.get("action") and step.get("observation"):
                    # Create ReAct-style intermediate step
                    action_obj = type('Action', (), {
                        'tool': step["action"],
                        'tool_input': step.get("action_input", "")
                    })()
                    intermediate_steps.append((action_obj, step["observation"]))

            return {
                "response": final_state.get("final_response", "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi"),
                "tools_used": final_state.get("tools_used", []),
                "reasoning_steps": final_state.get("reasoning_steps", []),
                "intermediate_steps": intermediate_steps,
                "session_id": session_id,
                "success": final_state.get("error_count", 0) == 0,
                "agent_type": "langgraph",
                "intent": final_state.get("intent", "unknown")
            }

        except Exception as e:
            logger.error(f"‚ùå LangGraph chat error: {e}")
            return {
                "response": self._get_error_response(str(e)),
                "tools_used": [],
                "reasoning_steps": [],
                "intermediate_steps": [],
                "session_id": session_id,
                "success": False,
                "error": str(e),
                "agent_type": "langgraph"
            }

    def clear_memory(self, session_id: str = "default") -> bool:
        """Clear conversation memory for session"""
        try:
            if session_id in self.memories:
                self.memories[session_id].clear()
                logger.info(f"‚úÖ Memory cleared for session: {session_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"‚ùå Error clearing memory: {e}")
            return False


# Enhanced Agent Manager for LangGraph
class LangGraphAgentManager:
    """Manages LangGraph agent instances and sessions"""

    def __init__(self):
        self.agent = ProductAdvisorLangGraphAgent()
        logger.info("‚úÖ LangGraphAgentManager initialized")

    def get_agent(self, session_id: str = "default") -> ProductAdvisorLangGraphAgent:
        """Get agent instance (single instance with session management)"""
        return self.agent

    def clear_session(self, session_id: str) -> bool:
        """Clear specific session"""
        return self.agent.clear_memory(session_id)

    def clear_all_sessions(self) -> bool:
        """Clear all sessions"""
        try:
            self.agent.memories.clear()
            logger.info("‚úÖ All sessions cleared")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error clearing all sessions: {e}")
            return False

    def get_active_sessions(self) -> List[str]:
        """Get list of active session IDs"""
        return list(self.agent.memories.keys())


# Global instances
langgraph_agent = ProductAdvisorLangGraphAgent()
langgraph_agent_manager = LangGraphAgentManager()


def get_langgraph_agent_manager() -> LangGraphAgentManager:
    """Get global LangGraph agent manager instance"""
    return langgraph_agent_manager
